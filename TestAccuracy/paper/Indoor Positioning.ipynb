{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indoor Positioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy local_records.json file to the same directory as this script, and then run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local_records.json data\n",
    "def load_data(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = file.read()\n",
    "    json_list = json.loads(data)\n",
    "    return json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = load_data('local_records.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split into 80% training data, 20% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shuffle the list\n",
    "random.Random(7).shuffle(json_list)\n",
    "# split into train data and test data\n",
    "ratio_of_train_data = 0.8\n",
    "cut_i = int(ratio_of_train_data * len(json_list))\n",
    "train = json_list[:cut_i]\n",
    "test = json_list[cut_i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and lables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature: [RSSI of AP_1_2GHZ, RSSI of AP_1_5GHZ, RSSI of AP_2_2GHZ, RSSI of AP_2_5GHZ, RSSI of AP_3_2GHZ, RSSI of AP_3_5GHZ, RSSI of AP_4_2GHZ, RSSI of AP_4_5GHZ]\n",
    "\n",
    "label: [x , y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features and lables from data\n",
    "index_map = {'SCSLAB_AP_1_2GHZ': 1, 'SCSLAB_AP_1_5GHZ': 2, 'SCSLAB_AP_2_2GHZ': 3,\\\n",
    "            'SCSLAB_AP_2_5GHZ': 4, 'SCSLAB_AP_3_2GHZ': 5, 'SCSLAB_AP_3_5GHZ': 6,\\\n",
    "            'SCSLAB_AP_4_2GHZ': 7, 'SCSLAB_AP_4_5GHZ': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_and_label(l):\n",
    "    feature_list = []\n",
    "    lable_list = []\n",
    "    for json_obj in l:\n",
    "        # extract lable\n",
    "        x = json_obj.get('ref_x')\n",
    "        y = json_obj.get('ref_y')\n",
    "        lable_list.append([x, y])\n",
    "        # extract feature\n",
    "        #feature = [0 for i in range(9)]\n",
    "        feature = [0 for i in range(8)]\n",
    "        #angle = json_obj.get('angle')\n",
    "        #feature[0] = angle\n",
    "        rssi_observations = json_obj.get('rssi_observations')\n",
    "        for observation in rssi_observations:\n",
    "            ssid = observation.get('SSID')\n",
    "            index = index_map[ssid]\n",
    "            feature[index - 1] = observation.get('RSSI')\n",
    "        feature_list.append(feature)\n",
    "    return np.array(feature_list, dtype='float64'), np.array(lable_list, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = extract_feature_and_label(train)\n",
    "test_data, test_labels = extract_feature_and_label(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rss = train_data.max(axis=0)\n",
    "min_rss = train_data.min(axis=0)\n",
    "\n",
    "train_data = (train_data - min_rss)/(max_rss - min_rss)\n",
    "test_data = (test_data - min_rss)/(max_rss - min_rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loss Function - Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.norm(y_true - y_pred, axis=1, ord='euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, activation='relu',input_shape=(shape,)))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(2))\n",
    "    model.compile(optimizer='adam', loss=euclidean_distance, metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(train_data.shape[1])\n",
    "history = model.fit(train_data, train_labels, epochs=50, batch_size=20, verbose=0, validation_split=0.2)\n",
    "loss, mse = model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(train_data.shape[1])\n",
    "history = model.fit(train_data, train_labels, epochs=50, batch_size=20, verbose=0, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_data[1:10]\n",
    "tmp = model.predict(data)\n",
    "print(tmp)\n",
    "print(tmp[0])\n",
    "print(tmp[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(10):\n",
    "    data = test_data[i]\n",
    "    start = time.time()\n",
    "    model.predict(np.array([data]))\n",
    "    print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('north_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in one fold\n",
    "fold_samples_num = len(json_list) // 10\n",
    "losses_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    print('Processing fold {}'.format(i + 1))\n",
    "    \n",
    "    val_start = i * fold_samples_num\n",
    "    val_end = (i + 1) * fold_samples_num\n",
    "    # prepare validation data\n",
    "    validation_dataset = json_list[val_start : val_end]\n",
    "    validation_data, validation_label = extract_feature_and_label(validation_dataset)\n",
    "    \n",
    "    # prepare training data\n",
    "    partial_training_dataset = np.concatenate([json_list[:val_start], json_list[val_end:]], axis=0)\n",
    "    training_data, training_label = extract_feature_and_label(partial_training_dataset)\n",
    "    \n",
    "    # build and train model\n",
    "    model = build_model(training_data.shape[1])\n",
    "    model.fit(training_data, training_label, epochs=50, batch_size=20, verbose=0)\n",
    "    loss, mse = model.evaluate(validation_data, validation_label)\n",
    "    losses_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses_list)\n",
    "print(np.mean(losses_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispatch_direction(json_obj, north_list, east_list, south_list, west_list):\n",
    "    angle = json_obj.get('angle')\n",
    "    if angle > 315 or angle <= 45:\n",
    "        north_list.append(json_obj)\n",
    "    elif angle > 45 and angle <= 135:\n",
    "        east_list.append(json_obj)\n",
    "    elif angle > 135 and angle < 225:\n",
    "        south_list.append(json_obj)\n",
    "    elif angle <= 315 and angle > 225:\n",
    "        west_list.append(json_obj)\n",
    "\n",
    "# extract all the data at point (x,y)\n",
    "def extract_data_at_one_point(x, y):\n",
    "    other = []\n",
    "    point_north = []\n",
    "    point_east = []\n",
    "    point_south = []\n",
    "    point_west = []\n",
    "    for json_obj in json_list:\n",
    "        ref_x = json_obj.get('ref_x')\n",
    "        ref_y = json_obj.get('ref_y')\n",
    "        if ref_x == x and ref_y == y:\n",
    "            dispatch_direction(json_obj, point_north, point_east, point_south, point_west)\n",
    "        else:\n",
    "            other.append(json_obj)\n",
    "    return other, point_north, point_east, point_south, point_west"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other, north_list, east_list, south_list, west_list = extract_data_at_one_point(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(north_list))\n",
    "print(len(east_list))\n",
    "print(len(south_list))\n",
    "print(len(west_list))\n",
    "print(len(other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_points = [(1,1), (1,5), (2,10), (5,4), (9,4), (9,1), (11,2), (6,1), (7,6), (4,9), (4,3), (6,3), (6,5), (4,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_train_and_test_data(data_list, direction_index):\n",
    "    # data_list[0] is the list 'other' \n",
    "    train_raw_data = data_list[0]\n",
    "    test_raw_data = []\n",
    "    for k in range(1, 5):\n",
    "        if k != direction_index:\n",
    "            train_raw_data += data_list[k]\n",
    "    # the list gonna to pick 50 samples randomly for test, and the rest should be added to train data\n",
    "    direction_list_for_test = data_list[direction_index]\n",
    "    random.shuffle(direction_list_for_test)\n",
    "    test_raw_data += direction_list_for_test[:50]\n",
    "    train_raw_data += direction_list_for_test[50:]\n",
    "    return train_raw_data, test_raw_data\n",
    "\n",
    "def normalize_data(train_data, test_data):\n",
    "    max_rss = train_data.max(axis=0)\n",
    "    min_rss = train_data.min(axis=0)\n",
    "    train_data = (train_data - min_rss)/(max_rss - min_rss)\n",
    "    test_data = (test_data - min_rss)/(max_rss - min_rss)\n",
    "    \n",
    "def calculate_error(model, test_data, test_labels):\n",
    "    prediction_list = model.predict(test_data)\n",
    "    errors = []\n",
    "    for i in range(len(prediction_list)):\n",
    "        pred = prediction_list[i]\n",
    "        real = test_labels[i]\n",
    "        e = sqrt(sum((pred - real)**2))\n",
    "        errors.append(e)\n",
    "    return mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CDABE66D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CDAB373940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n"
     ]
    }
   ],
   "source": [
    "point_error_map = dict()\n",
    "# for all the test points\n",
    "for point in test_points:\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    data_list = extract_data_at_one_point(x,y)\n",
    "    error_list = []\n",
    "    # repeat 1000 times\n",
    "    for i in range(1000):\n",
    "        direction_error_list = []\n",
    "        # repeat for N, E, S, W\n",
    "        for j in range(1, 5):\n",
    "            train_raw_data, test_raw_data = get_raw_train_and_test_data(data_list, j)\n",
    "            \n",
    "            # extract features from raw json data\n",
    "            train_data, train_labels = extract_feature_and_label(train_raw_data)\n",
    "            test_data, test_labels = extract_feature_and_label(test_raw_data)\n",
    "            \n",
    "            # normalize train data and test data\n",
    "            normalize_data(train_data, test_data)\n",
    "            \n",
    "            # build and train the model\n",
    "            model = build_model(train_data.shape[1])\n",
    "            model.fit(train_data, train_labels, epochs=50, batch_size=20, verbose=0, validation_split=0.2)\n",
    "            \n",
    "            # calculate the mean error\n",
    "            e = calculate_error(model, test_data, test_labels)\n",
    "            direction_error_list.append(e)\n",
    "        error_list.append(mean(direction_error_list))\n",
    "    point_error_map[point] = mean(error_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key: point \n",
    "# value: raw json data at the point\n",
    "point_data_map = dict()\n",
    "for json_obj in json_list:\n",
    "    ref_x = json_obj.get('ref_x')\n",
    "    ref_y = json_obj.get('ref_y')\n",
    "    point = (ref_x, ref_y)\n",
    "    if point_data_map.get(point) is None:\n",
    "        point_data_map[point] = [json_obj]\n",
    "    else:\n",
    "        point_data_map[point].append(json_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "print(len(json_list) // 10 // 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test_data():\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for point in point_data_map:\n",
    "        data = point_data_map[point]\n",
    "        random.shuffle(data)\n",
    "        test_data += data[:75]\n",
    "        train_data += data[75:]\n",
    "    return train_data, test_data\n",
    "\n",
    "def normalize_data(train_data, test_data):\n",
    "    max_rss = train_data.max(axis=0)\n",
    "    min_rss = train_data.min(axis=0)\n",
    "    train_data = (train_data - min_rss)/(max_rss - min_rss)\n",
    "    test_data = (test_data - min_rss)/(max_rss - min_rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 0s 1ms/step\n",
      "127/127 [==============================] - 0s 1ms/step\n",
      "127/127 [==============================] - 0s 1ms/step\n",
      "127/127 [==============================] - 0s 1ms/step\n",
      "127/127 [==============================] - 0s 1ms/step\n",
      "127/127 [==============================] - 0s 2ms/step\n",
      "127/127 [==============================] - 0s 1ms/step\n",
      "127/127 [==============================] - 0s 2ms/step\n",
      "127/127 [==============================] - 0s 2ms/step\n",
      "127/127 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "point_error_map_list = []\n",
    "\n",
    "for _ in range(10):\n",
    "    train_raw_data, test_raw_data = get_train_and_test_data()\n",
    "    \n",
    "    # extract features from raw json data\n",
    "    train_data, train_labels = extract_feature_and_label(train_raw_data)\n",
    "    test_data, test_labels = extract_feature_and_label(test_raw_data)\n",
    "    \n",
    "    # normalize train data and test data\n",
    "    normalize_data(train_data, test_data)\n",
    "    \n",
    "    # build and train the model\n",
    "    model = build_model(train_data.shape[1])\n",
    "    model.fit(train_data, train_labels, epochs=50, batch_size=20, verbose=0, validation_split=0.2)\n",
    "    \n",
    "    # calculate error\n",
    "    prediction_list = model.predict(test_data)\n",
    "    point_error_list_map = dict()\n",
    "    for i in range(len(prediction_list)):\n",
    "        pred = prediction_list[i]\n",
    "        real = test_labels[i]\n",
    "        e = sqrt(sum((pred - real)**2))\n",
    "        label = (real[0], real[1])\n",
    "        if point_error_list_map.get(label) is None:\n",
    "            point_error_list_map[label] = [e]\n",
    "        else:\n",
    "            point_error_list_map[label].append(e)\n",
    "    \n",
    "    # calculate mean error on every points\n",
    "    for point in point_error_list_map:\n",
    "        error_list = point_error_list_map[point]\n",
    "        point_error_list_map[point] = mean(error_list)\n",
    "    \n",
    "    point_error_map_list.append(point_error_list_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_mean_error = dict()\n",
    "for point_error_map in point_error_map_list:\n",
    "    for point in point_error_map:\n",
    "        e = point_error_map[point]\n",
    "        if point_mean_error.get(point) is None:\n",
    "            point_mean_error[point] = [e]\n",
    "        else:\n",
    "            point_mean_error[point].append(e)\n",
    "for point in point_mean_error:\n",
    "    point_mean_error[point] = mean(point_mean_error[point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(4.0, 6.0): 0.5992055081591194,\n",
       " (3.0, 6.0): 0.6398665260675199,\n",
       " (2.0, 6.0): 0.6686195892300383,\n",
       " (1.0, 6.0): 0.5870194576153228,\n",
       " (4.0, 5.0): 0.45214113302781683,\n",
       " (3.0, 5.0): 0.5344676827260528,\n",
       " (1.0, 5.0): 0.8038753085604944,\n",
       " (1.0, 4.0): 0.5994764553762603,\n",
       " (1.0, 3.0): 1.0532681991922337,\n",
       " (4.0, 4.0): 0.4403697163023755,\n",
       " (4.0, 3.0): 0.5039032405400933,\n",
       " (5.0, 3.0): 0.3705735190473584,\n",
       " (6.0, 5.0): 0.7439063145745783,\n",
       " (5.0, 5.0): 0.352239484227578,\n",
       " (5.0, 4.0): 0.43353482723869713,\n",
       " (6.0, 3.0): 0.8728468710726418,\n",
       " (6.0, 4.0): 0.834757951463432,\n",
       " (9.0, 5.0): 0.922069487246144,\n",
       " (6.0, 6.0): 0.31201846366311004,\n",
       " (8.0, 5.0): 0.6109045487946602,\n",
       " (7.0, 5.0): 0.6760216872612765,\n",
       " (8.0, 4.0): 0.6587782274874333,\n",
       " (7.0, 6.0): 0.4368412688161915,\n",
       " (9.0, 4.0): 0.7551004310226614,\n",
       " (1.0, 1.0): 0.9111117611174286,\n",
       " (2.0, 1.0): 0.5046187864726434,\n",
       " (3.0, 1.0): 0.7306741185937016,\n",
       " (1.0, 2.0): 0.7735588838484987,\n",
       " (10.0, 4.0): 0.6763472681378719,\n",
       " (10.0, 3.0): 0.5111982858491493,\n",
       " (11.0, 2.0): 0.4558682748044551,\n",
       " (10.0, 2.0): 0.5188670484886846,\n",
       " (9.0, 3.0): 0.495090421267731,\n",
       " (9.0, 2.0): 0.5279266623134219,\n",
       " (10.0, 1.0): 0.5362200379927136,\n",
       " (9.0, 1.0): 0.5310999969368966,\n",
       " (6.0, 1.0): 0.5711163167308975,\n",
       " (7.0, 1.0): 0.8968227544342675,\n",
       " (8.0, 1.0): 0.5748697725867451,\n",
       " (7.0, 7.0): 1.5510339456088387,\n",
       " (6.0, 8.0): 0.7338075790223607,\n",
       " (5.0, 9.0): 0.5306046893295454,\n",
       " (5.0, 8.0): 0.5819114971321774,\n",
       " (4.0, 8.0): 2.240981295852279,\n",
       " (4.0, 9.0): 3.963657065612687,\n",
       " (3.0, 11.0): 7.385261839584271,\n",
       " (3.0, 10.0): 4.3454835594553085,\n",
       " (2.0, 11.0): 6.383997917961466,\n",
       " (2.0, 10.0): 4.6342812351222396,\n",
       " (1.0, 10.0): 4.8618119494373735,\n",
       " (2.0, 9.0): 3.7276427846025797,\n",
       " (1.0, 9.0): 3.827906151827873,\n",
       " (1.0, 8.0): 4.011370880062524,\n",
       " (1.0, 7.0): 3.3952043554706473}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error for all points: 1.4120769080438957\n"
     ]
    }
   ],
   "source": [
    "all_p_me = []\n",
    "for e in point_mean_error.values():\n",
    "    all_p_me.append(e)\n",
    "print(\"Mean error for all points: \" + str(mean(all_p_me)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error after remove coner points: 0.6389431162646771\n"
     ]
    }
   ],
   "source": [
    "p_me = []\n",
    "for e in point_mean_error.values():\n",
    "    if e < 2:\n",
    "        p_me.append(e)\n",
    "print(\"Mean error after remove coner points: \" + str(mean(p_me)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1b5f54e4835e6e96295ff4f9b2e3106cd88f1d758fc4bd43ae981ee911efc60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
